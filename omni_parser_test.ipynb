{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGXUKQtZqFcy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"FLAGS_use_mkldnn\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jl5f-SUbmp96",
    "outputId": "786dfce9-992c-4ef0-a98a-1253856d7bdd"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/microsoft/OmniParser.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4M3uW2Prm5no",
    "outputId": "47f056ab-bd13-4c86-e55f-0a4d06b5a062"
   },
   "outputs": [],
   "source": [
    "%cd OmniParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KYzJAAajnKQ2",
    "outputId": "be99d95c-786d-40af-af1f-a1b9aaac151a"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXjdoWC2nOp6",
    "outputId": "7139f973-e31c-4028-c844-6a06d54a1f78"
   },
   "outputs": [],
   "source": [
    "!mkdir weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-oJlrM4o_3s",
    "outputId": "013aa568-55b6-4fb0-ac9f-ef99386bbd87"
   },
   "outputs": [],
   "source": [
    "!python -m pip uninstall -y huggingface-hub -q\n",
    "!python -m pip install -q \"huggingface-hub==0.23.4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-hLXk6wpNud",
    "outputId": "85699118-b98f-4eff-b11f-a7946b4016b6"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from huggingface_hub import login\n",
    "\n",
    "token = getpass()\n",
    "login(token=token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402,
     "referenced_widgets": [
      "4ec0cf441d344030a4d192b144cb16ec",
      "0d6e701ab30747d5a95478433f82ac5d",
      "963c359b920f4d0a98d3805a18dd69c5",
      "44eab9e3f2ee47038c5b1ed305a64c48",
      "10a39b3af6a14cdea705421035ee6ec7",
      "5df424794c3a4a8ba763f763d16529bd",
      "ab6370d8ac9f43d4a9b7d9783c0dc67a",
      "cea4a76d3e204ed4856552fc685ee336",
      "f6e79b12e6e546c8a7a96b57dc2294c0",
      "1ce38215c4114f59977292647772be9b",
      "115da8fdcad1413e953bf2eebc3a35ca",
      "db167697a148481b96893abaf92cca98",
      "e61f263047c9491f9c8d0d8835b6f73c",
      "820b7cf26c644eb3aae91ba454e1019e",
      "824a9d982e2d43119eef9de2a8cfac28",
      "dd9c8ff35258469d9dc8205a74d290ec",
      "0b1ed18f70e7401fb82b320e71979361",
      "d0b880e95dd24894aed340b4b9bb2001",
      "58a0975d313b4e579ad8900dfdec9570",
      "fad209d52dba40f4adbf71b4f541afa2",
      "c18ee850aad047368aa2163db93adc1d",
      "c92f690d1cc842af9ff8f421fffbb7d3",
      "cea7776ac9514385905123b29caf0763",
      "fbe55a582a8e4324b8254badf4494674",
      "4520554147494610bc6793d8d67e0e48",
      "cc494951354e4de59154561199701e81",
      "cb65d9938c274fc89bdfec4846995f90",
      "5b61aa95ac644e09b6d326b855e2d813",
      "e5f7a1f0124147b1ad4d8a7916328492",
      "aa8b032fbe964979beface5234fda8bc",
      "dc89f7e0e4024b6daddaf4e8ce96adcc",
      "c778d06471624605aa77132f6b510302",
      "2b55d1e858c24e93a4ac53f8935dee78"
     ]
    },
    "id": "khRKj_dxpxbY",
    "outputId": "2a9c2e63-351d-4a6a-a181-d8a79bfce9b0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo = \"microsoft/OmniParser-v2.0\"\n",
    "out_dir = \"weights\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "files = [\n",
    "    \"icon_detect/train_args.yaml\",\n",
    "    \"icon_detect/model.pt\",\n",
    "    \"icon_detect/model.yaml\",\n",
    "    \"icon_caption/config.json\",\n",
    "    \"icon_caption/generation_config.json\",\n",
    "    \"icon_caption/model.safetensors\",\n",
    "]\n",
    "\n",
    "for f in files:\n",
    "    path = hf_hub_download(\n",
    "        repo_id=repo,\n",
    "        filename=f,\n",
    "        local_dir=out_dir,\n",
    "        local_dir_use_symlinks=False,\n",
    "        force_download=True,      \n",
    "        resume_download=False     \n",
    ")\n",
    "    \n",
    "    print(\"baixado:\", path)\n",
    "\n",
    "\n",
    "!mv OmniParser/weights/icon_caption OmniParser/weights/icon_caption_florence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XtM-E1HtBXV",
    "outputId": "5de52993-c495-431f-e47e-3a207bba4d5b"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "files = api.list_repo_files(\"microsoft/OmniParser-v2.0\")\n",
    "\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWqY-S7wqqFw",
    "outputId": "5e25d8e9-1f19-48bb-bb99-90b322020846"
   },
   "outputs": [],
   "source": [
    "!ls OmniParser/weights/\n",
    "!ls OmniParser/weights/icon_caption_florence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "plOhT9K7qPIG",
    "outputId": "842ea6c7-ae2e-4962-8ed3-066ed40513d6"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/OmniParser\")\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = \"OmniParser/weights/icon_detect/model.pt\"\n",
    "device = \"cuda\"\n",
    "\n",
    "som_model = YOLO(model_path).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuJqolVzkVnD",
    "outputId": "f501ed27-33c7-4fd3-a572-c31e52716982"
   },
   "outputs": [],
   "source": [
    "!python -m pip install -q -U \"transformers==4.46.3\" \"tokenizers==0.20.3\" \"accelerate==0.34.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "sU-gPE43qbjy",
    "outputId": "0a68e284-2642-491f-a8bd-ef3fa5938c9b"
   },
   "outputs": [],
   "source": [
    "# two choices for caption model: fine-tuned blip2 or florence2\n",
    "import importlib\n",
    "# import util.utils\n",
    "# importlib.reload(utils)\n",
    "from util.utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model\n",
    "caption_model_processor = get_caption_model_processor(model_name=\"florence2\", model_name_or_path=\"OmniParser/weights/icon_caption_florence/\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYG-emHUtzq9"
   },
   "outputs": [],
   "source": [
    "som_model.device, type(som_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    start = start or Path.cwd()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"requirements.txt\").exists() and (p / \"util\" / \"utils.py\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "\n",
    "    )\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "print(\"REPO_ROOT =\", REPO_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLKVRDCRt4nc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os, importlib\n",
    "\n",
    "import os\n",
    "os.chdir(REPO_ROOT) \n",
    "\n",
    "from util.utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "device = 'cuda'\n",
    "model_path='OmniParser/weights/icon_detect/model.pt'\n",
    "\n",
    "caption_model_processor = get_caption_model_processor(model_name=\"florence2\", model_name_or_path=\"OmniParser/weights/icon_caption_florence/\", device=device)\n",
    "\n",
    "som_model = get_yolo_model(model_path)\n",
    "\n",
    "#image_path = 'imgs/google_page.png'\n",
    "# image_path = 'imgs/windows_home.png'\n",
    "# image_path = 'imgs/windows_multitab.png'\n",
    "# image_path = 'imgs/omni3.jpg'\n",
    "# image_path = 'imgs/ios.png'\n",
    "image_path = 'image.png'\n",
    "# image_path = 'imgs/excel2.png'\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image_rgb = image.convert('RGB')\n",
    "print('image size:', image.size)\n",
    "\n",
    "box_overlay_ratio = max(image.size) / 3200\n",
    "draw_bbox_config = {\n",
    "    'text_scale': 0.8 * box_overlay_ratio,\n",
    "    'text_thickness': max(int(2 * box_overlay_ratio), 1),\n",
    "    'text_padding': max(int(3 * box_overlay_ratio), 1),\n",
    "    'thickness': max(int(3 * box_overlay_ratio), 1),\n",
    "}\n",
    "BOX_TRESHOLD = 0.05 #mexi aqui\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "ocr_bbox_rslt, is_goal_filtered = check_ocr_box(image_path, display_img = False, output_bb_format='xyxy', goal_filtering=None, easyocr_args={'paragraph': False, 'text_threshold':0.9}, use_paddleocr=False)\n",
    "text, ocr_bbox = ocr_bbox_rslt\n",
    "cur_time_ocr = time.time()\n",
    "\n",
    "dino_labled_img, label_coordinates, parsed_content_list = get_som_labeled_img(image_path, som_model, BOX_TRESHOLD = BOX_TRESHOLD, output_coord_in_ratio=True, ocr_bbox=ocr_bbox,draw_bbox_config=draw_bbox_config, caption_model_processor=caption_model_processor, ocr_text=text,use_local_semantics=True, iou_threshold=0.7, scale_img=False, batch_size=128)\n",
    "cur_time_caption = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-adKWWhEuS_f"
   },
   "outputs": [],
   "source": [
    "# plot dino_labled_img it is in base64\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "image = Image.open(io.BytesIO(base64.b64decode(dino_labled_img)))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.imshow(image)\n",
    "print(len(parsed_content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFHRfyA_umQT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(parsed_content_list)\n",
    "df['ID'] = range(len(df))\n",
    "\n",
    "display(df.drop(columns=[\"source\"], errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOxUMCaj_70q"
   },
   "outputs": [],
   "source": [
    "# retornar isso num array\n",
    "\n",
    "#parsed_content_list\n",
    "parsed_with_id = []\n",
    "for i, e in enumerate(parsed_content_list):\n",
    "    e_clean = dict(e)\n",
    "    e_clean.pop(\"source\", None)\n",
    "    e_clean[\"id\"] = i\n",
    "    parsed_with_id.append(e_clean)\n",
    "\n",
    "parsed_with_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqAnVJunvEUa"
   },
   "source": [
    "# THE AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zixq7EpQvAYL"
   },
   "outputs": [],
   "source": [
    "key = 'REDACTED_GROQ_KEY_2hBRP39kIlkeASgHtOqYWGdyb3FYprorcZHbjUeGZQAxwh2IxJha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLE3Yn5KvHW6"
   },
   "outputs": [],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdFXlT49vnbn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=key)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a UI automation agent for Android.\n",
    "\n",
    "Goal:\n",
    "Given a task and the current screen state (a list of UI elements with IDs, text, interactivity, and bounding boxes), choose the single best NEXT action to move toward completing the task.\n",
    "\n",
    "The screen state includes spatial information:\n",
    "- Each element has a bbox = [x1, y1, x2, y2] in normalized screen coordinates (0..1).\n",
    "- Use bbox to reason about layout (top bar, bottom navigation, center content), proximity, and likely UI roles.\n",
    "\n",
    "You must output ONLY a JSON object (no markdown, no extra text).\n",
    "\n",
    "Allowed actions:\n",
    "- \"tap\": tap a UI element by target_id (must be interactive=true)\n",
    "- \"type\": type text into a UI element by target_id (must be interactive=true and suitable for input)\n",
    "- \"swipe\": swipe the screen in a direction (\"up\",\"down\",\"left\",\"right\") when needed to reveal content\n",
    "- \"back\": press Android back\n",
    "- \"wait\": wait briefly if loading is likely\n",
    "- \"done\": task completed\n",
    "\n",
    "Spatial heuristics (use when helpful):\n",
    "- Top region (small y): often status bar / search bars / page titles.\n",
    "- Bottom region (large y): often navigation bar / action buttons.\n",
    "- Large center elements: usually main content.\n",
    "- If multiple elements match the same text intent, prefer the most plausible by position/size.\n",
    "\n",
    "Rules:\n",
    "1) Choose exactly ONE next action.\n",
    "2) Prefer \"tap\" on an element whose content best matches the task intent (exact match > partial match > semantic match).\n",
    "3) Use bbox to disambiguate: e.g., prefer a search icon near the top over a random icon in the middle.\n",
    "4) If you need to type, first tap the input field, then type in the next step. (One action per step.)\n",
    "5) Never invent IDs. Use only IDs provided in the screen state.\n",
    "6) If no good action exists, choose \"swipe\" (usually \"up\") or \"back\".\n",
    "7) Keep \"reason\" short (max 1 sentence).\n",
    "\n",
    "Return JSON with this schema:\n",
    "{\n",
    "  \"action\": \"tap|type|swipe|back|wait|done\",\n",
    "  \"target_id\": number|null,\n",
    "  \"text\": string|null,\n",
    "  \"direction\": \"up|down|left|right\"|null,\n",
    "  \"confidence\": number,\n",
    "  \"reason\": string\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def choose_next_action(task: str, elements: list, screen_wh=(1080, 2408)):\n",
    "    payload = {\n",
    "        \"task\": task,\n",
    "        \"screen\": {\"width\": screen_wh[0], \"height\": screen_wh[1]},\n",
    "        \"elements\": elements\n",
    "    }\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(payload, ensure_ascii=False)}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    raw = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # Tenta parsear o JSON\n",
    "    try:\n",
    "        action = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback: tenta extrair o primeiro {...}\n",
    "        start = raw.find(\"{\")\n",
    "        end = raw.rfind(\"}\")\n",
    "        action = json.loads(raw[start:end+1])\n",
    "\n",
    "    return action\n",
    "\n",
    "task = \"Make a call to the first contact of the list\"\n",
    "print(parsed_content_list)\n",
    "\n",
    "action = choose_next_action(task, state[\"elements\"], screen_wh=(1080, 2408))\n",
    "print(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sahn4oTXzXBY"
   },
   "outputs": [],
   "source": [
    "import io, json, base64\n",
    "from PIL import Image\n",
    "from groq import Groq\n",
    "\n",
    "GROQ_MODEL = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
    "\n",
    "# Prompt mínimo (sem system prompt)\n",
    "def build_prompt(task: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an Android UI automation agent. You will receive ONLY a screenshot.\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Decide the single best NEXT action to move toward completing the task.\n",
    "Output ONLY a JSON object (no markdown, no extra text).\n",
    "\n",
    "Allowed actions:\n",
    "- \"tap\": tap somewhere on the screen\n",
    "- \"type\": type text (assume keyboard is ready OR you tapped an input previously)\n",
    "- \"swipe\": swipe the screen (\"up\",\"down\",\"left\",\"right\")\n",
    "- \"back\": press Android back\n",
    "- \"wait\": wait briefly if loading is likely\n",
    "- \"done\": task completed\n",
    "\n",
    "Rules:\n",
    "1) Choose exactly ONE next action.\n",
    "2) If you tap, provide either a tap_bbox in normalized coordinates (0..1).\n",
    "3) If you type, fill the \"text\" field.\n",
    "4) Keep \"reason\" short (max 1 sentence).\n",
    "\n",
    "Return JSON with this schema:\n",
    "{{\n",
    "  \"action\": \"tap|type|swipe|back|wait|done\",\n",
    "  \"text\": string|null,\n",
    "  \"direction\": \"up|down|left|right\"|null,\n",
    "  \"confidence\": number,\n",
    "  \"reason\": string,\n",
    "  \"tap_bbox\": [number, number, number, number]|null\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "def pil_to_data_url(image: Image.Image) -> str:\n",
    "    buf = io.BytesIO()\n",
    "    image.save(buf, format=\"PNG\")\n",
    "    b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "    return f\"data:image/png;base64,{b64}\"\n",
    "\n",
    "def choose_next_action_image_only(task: str, image: Image.Image, max_new_tokens=300):\n",
    "    prompt = build_prompt(task)\n",
    "    data_url = pil_to_data_url(image.convert(\"RGB\"))\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=GROQ_MODEL,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }],\n",
    "        temperature=0.0,\n",
    "        max_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    raw = resp.choices[0].message.content.strip()\n",
    "\n",
    "    # parse robusto\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        s = raw.find(\"{\")\n",
    "        e = raw.rfind(\"}\")\n",
    "        return json.loads(raw[s:e+1])\n",
    "\n",
    "\n",
    "img = Image.open(\"/content/image.png\")\n",
    "action = choose_next_action_image_only(\"Make a call to the first contact of the list\", img)\n",
    "print(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsl6dMgmC9OA"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def draw_action_tap_bbox(\n",
    "    image: Image.Image,\n",
    "    action: dict,\n",
    "    color=(255, 0, 0),\n",
    "    width=4,\n",
    "    also_draw_point=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draws action[\"tap_bbox\"] (normalized 0..1) on a copy of `image`.\n",
    "\n",
    "    Expected action format (image-only version):\n",
    "      action[\"tap_bbox\"]  -> [x1, y1, x2, y2]  (normalized)\n",
    "      action[\"tap_point\"] -> [x, y]            (normalized, optional)\n",
    "\n",
    "    Returns: PIL.Image (new image with drawing)\n",
    "    \"\"\"\n",
    "    img = image.convert(\"RGB\").copy()\n",
    "    W, H = img.size\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    tap_bbox = action.get(\"tap_bbox\", None)\n",
    "    if tap_bbox is not None:\n",
    "        if not (isinstance(tap_bbox, (list, tuple)) and len(tap_bbox) == 4):\n",
    "            raise ValueError(f\"tap_bbox must be a list of 4 numbers, got: {tap_bbox}\")\n",
    "\n",
    "        x1n, y1n, x2n, y2n = tap_bbox\n",
    "\n",
    "        # clamp\n",
    "        x1n = max(0.0, min(1.0, float(x1n)))\n",
    "        y1n = max(0.0, min(1.0, float(y1n)))\n",
    "        x2n = max(0.0, min(1.0, float(x2n)))\n",
    "        y2n = max(0.0, min(1.0, float(y2n)))\n",
    "\n",
    "        # convert to px\n",
    "        x1 = int(round(x1n * W))\n",
    "        y1 = int(round(y1n * H))\n",
    "        x2 = int(round(x2n * W))\n",
    "        y2 = int(round(y2n * H))\n",
    "\n",
    "        # ensure valid ordering\n",
    "        left, right = sorted([x1, x2])\n",
    "        top, bottom = sorted([y1, y2])\n",
    "\n",
    "        # draw rectangle (thicker by drawing multiple times)\n",
    "        for k in range(width):\n",
    "            draw.rectangle([left - k, top - k, right + k, bottom + k], outline=color)\n",
    "\n",
    "        # optionally draw center point of bbox\n",
    "        if also_draw_point:\n",
    "            cx = (left + right) // 2\n",
    "            cy = (top + bottom) // 2\n",
    "            r = max(3, width + 2)\n",
    "            draw.ellipse([cx - r, cy - r, cx + r, cy + r], outline=color, width=width)\n",
    "\n",
    "    # If no bbox, but a point exists, draw the point\n",
    "    tap_point = action.get(\"tap_point\", None)\n",
    "    if tap_bbox is None and tap_point is not None and also_draw_point:\n",
    "        if not (isinstance(tap_point, (list, tuple)) and len(tap_point) == 2):\n",
    "            raise ValueError(f\"tap_point must be a list of 2 numbers, got: {tap_point}\")\n",
    "\n",
    "        xn, yn = tap_point\n",
    "        xn = max(0.0, min(1.0, float(xn)))\n",
    "        yn = max(0.0, min(1.0, float(yn)))\n",
    "\n",
    "        x = int(round(xn * W))\n",
    "        y = int(round(yn * H))\n",
    "        r = max(6, width + 4)\n",
    "        draw.ellipse([x - r, y - r, x + r, y + r], outline=color, width=width)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "out = draw_action_tap_bbox(img, action)\n",
    "display(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxiFvG1eGFbz"
   },
   "outputs": [],
   "source": [
    "## outro fluxo\n",
    "\n",
    "STATE_PROMPT = \"\"\"\n",
    "You are a vision model extracting an Android UI state from a screenshot.\n",
    "\n",
    "Return ONLY valid JSON. No markdown. No extra text.\n",
    "\n",
    "Task: Convert the screenshot into an OmniParser-like state.\n",
    "\n",
    "Output schema:\n",
    "{\n",
    "  \"screen\": {\"width\": int, \"height\": int},\n",
    "  \"elements\": [\n",
    "    {\n",
    "      \"id\": int,\n",
    "      \"type\": \"text|icon|button|input|image|chip|toggle|checkbox|list_item|nav|unknown\",\n",
    "      \"text\": string,\n",
    "      \"interactive\": boolean,\n",
    "      \"bbox\": [x1, y1, x2, y2],\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- bbox is normalized ratios in [0,1] relative to (width,height): [x1,y1,x2,y2], top-left to bottom-right.\n",
    "- Assign ids starting from 0 in reading order: top-to-bottom, left-to-right.\n",
    "- \"text\" must be the visible label (for icons/buttons use the best guessed label; if unknown use \"\").\n",
    "- interactive=true only if it looks tappable/clickable (buttons, nav items, icons, input fields, toggles, list items).\n",
    "- Do NOT invent hidden elements. Only what is visible.\n",
    "- Include at most 60 elements. Prefer higher-salience elements (buttons, fields, nav, list items) over decorative text.\n",
    "- screen width/height must match the screenshot pixel size.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxduQJ5WJdx6"
   },
   "outputs": [],
   "source": [
    "import io, base64, json\n",
    "from PIL import Image\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "def vlm_extract_state(image: Image.Image, model=\"meta-llama/llama-4-maverick-17b-128e-instruct\", max_tokens=1800):\n",
    "    buf = io.BytesIO()\n",
    "    image.save(buf, format=\"PNG\")\n",
    "    b64 = base64.b64encode(buf.getvalue()).decode()\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{b64}\"}},\n",
    "                {\"type\": \"text\", \"text\": STATE_PROMPT}\n",
    "            ],\n",
    "        }],\n",
    "        temperature=0.0,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    raw = resp.choices[0].message.content.strip()\n",
    "\n",
    "    # parse robusto\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        s = raw.find(\"{\")\n",
    "        e = raw.rfind(\"}\")\n",
    "        return json.loads(raw[s:e+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0q7DxzhJgyX"
   },
   "outputs": [],
   "source": [
    "img = Image.open(\"/content/image.png\").convert(\"RGB\")\n",
    "state = vlm_extract_state(img)\n",
    "print(state[\"elements\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrwfP8mOKVyV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def ensure_state_list(state):\n",
    "    # se veio como string JSON, converte\n",
    "    if isinstance(state, str):\n",
    "        state = json.loads(state)\n",
    "\n",
    "    # alguns casos: dict com chave tipo {\"elements\":[...]}\n",
    "    if isinstance(state, dict):\n",
    "        if \"elements\" in state:\n",
    "            state = state[\"elements\"]\n",
    "        elif \"state\" in state:\n",
    "            state = state[\"state\"]\n",
    "        else:\n",
    "            # tenta pegar primeiro valor que seja lista\n",
    "            for v in state.values():\n",
    "                if isinstance(v, list):\n",
    "                    state = v\n",
    "                    break\n",
    "\n",
    "    if not isinstance(state, list):\n",
    "        raise TypeError(f\"state precisa ser list, veio: {type(state)}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "def draw_all_state_bboxes(img: Image.Image, state, only_interactive=False):\n",
    "    state = ensure_state_list(state)\n",
    "\n",
    "    out = img.convert(\"RGB\").copy()\n",
    "    for e in state:\n",
    "        if not isinstance(e, dict):\n",
    "            continue  # ignora strings etc\n",
    "\n",
    "        bbox = e.get(\"bbox\", None)\n",
    "        if bbox is None:\n",
    "            continue\n",
    "\n",
    "        if only_interactive and not e.get(\"interactive\", False):\n",
    "            continue\n",
    "\n",
    "        action = {\"tap_bbox\": bbox}\n",
    "        out = draw_action_tap_bbox(out, action, color=(255, 0, 0), width=3, also_draw_point=False)\n",
    "\n",
    "    return out\n",
    "\n",
    "# uso:\n",
    "# img = Image.open(\"...\")  # ou sua img já carregada\n",
    "out = draw_all_state_bboxes(img, state, only_interactive=False)\n",
    "display(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlHIBejTOkGp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArpcWjYfOkAU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
